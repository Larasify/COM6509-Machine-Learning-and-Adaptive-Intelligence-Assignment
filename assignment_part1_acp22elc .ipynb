{"cells":[{"cell_type":"markdown","metadata":{"id":"IV6UGwSdcYHf"},"source":["Enter your username (used for marking):"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"rybkkNgScYHi"},"outputs":[],"source":["username = 'acp22elc'"]},{"cell_type":"markdown","metadata":{"id":"j0FiyuFscYHl"},"source":["# COM4509/6509 Coursework Part 1\n","\n","Hello,\n","This is the first of the two parts. Each part accounts for 50\\% of the overall coursework mark.\n","\n","### What to submit\n","\n","- You need to submit **two jupyter notebooks** (not zipped together), named:\n","\n","```\n","assignment_part1_[username].ipynb\n","assignment_part2_[username].ipynb\n","```\n","\n","replacing `[username]` with your username, e.g. `abc18de`.\n","\n","- Please do not upload the data files used in this Notebook. We just want the two python notebooks.\n","\n","### Assessment Criteria \n","\n","- The marks are indicated for each part: You'll get marks for correct code that does what is asked and gets the right answer. These contribute 45.\n","- There are also 5 marks for \"Code quality\" (includes both readability and efficiency).\n","\n","### Late submissions\n","\n","We follow the department's guidelines about late submissions, Undergraduate [handbook link](https://sites.google.com/sheffield.ac.uk/comughandbook/your-study/assessment/late-submission). PGT [handbook link](https://sites.google.com/sheffield.ac.uk/compgtstudenthandbook/home/your-study/assessment/late-submission).\n","\n","### Use of unfair means\n","\"Any form of unfair means is treated as a serious academic offence and action may be taken under the Discipline Regulations.\" (from the students Handbook).\n","\n","# A dataset of air quality\n","\n","We are going to use a dataset of air pollution measurements in Beijing archived by the UCI. To read about the dataset visit the [UCI page](https://archive.ics.uci.edu/ml/datasets/Beijing+Multi-Site+Air-Quality+Data).\n","\n","We are going to:\n","  1. Preprocess the data\n","  2. Build our own Lasso-regression\n","  3. Use sklearn's tools to perform regression on the data\n","  \n","Let's get started"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"NWxSewmucYHp"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn import linear_model\n","from sklearn import linear_model\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.ensemble import RandomForestRegressor\n","import urllib\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"WWSGSGFfcYHs"},"source":["We will be trying to predict the pollution (PM2.5) using:\n","- temperature ('TEMP')\n","- pressure ('PRES')\n","- dew-point temperature ('DEWP')\n","- precipitation ('RAIN')\n","- wind direction ('wd')\n","- wind speed ('WSPM')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"QujKMUPtcYHu"},"outputs":[{"data":{"text/plain":["('./data.csv', <http.client.HTTPMessage at 0x14ea99880>)"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["urllib.request.urlretrieve('https://drive.google.com/uc?id=1m1g4Xn1wMAGV_EU0Nh1HTI1ogA3-tqJk&export=download', './data.csv')"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"2qOVpf72cYHv"},"outputs":[{"name":"stdout","output_type":"stream","text":["       PM2.5  year  month  day  hour  PM10  SO2   NO2     CO    O3  TEMP  \\\n","No                                                                         \n","1        9.0  2013      3    1     0   9.0  6.0  17.0  200.0  62.0   0.3   \n","2       11.0  2013      3    1     1  11.0  7.0  14.0  200.0  66.0  -0.1   \n","3        8.0  2013      3    1     2   8.0  NaN  16.0  200.0  59.0  -0.6   \n","4        8.0  2013      3    1     3   8.0  3.0  16.0    NaN   NaN  -0.7   \n","5        8.0  2013      3    1     4   8.0  3.0   NaN  300.0  36.0  -0.9   \n","...      ...   ...    ...  ...   ...   ...  ...   ...    ...   ...   ...   \n","35060   11.0  2017      2   28    19  32.0  3.0  24.0  400.0  72.0  12.5   \n","35061   13.0  2017      2   28    20  32.0  3.0  41.0  500.0  50.0  11.6   \n","35062   14.0  2017      2   28    21  28.0  4.0  38.0  500.0  54.0  10.8   \n","35063   12.0  2017      2   28    22  23.0  4.0  30.0  400.0  59.0  10.5   \n","35064   13.0  2017      2   28    23  19.0  4.0  38.0  600.0  49.0   8.6   \n","\n","         PRES  DEWP  RAIN   wd  WSPM        station  \n","No                                                   \n","1      1021.9 -19.0   0.0  WNW   2.0  Wanshouxigong  \n","2      1022.4 -19.3   0.0  WNW   4.4  Wanshouxigong  \n","3      1022.6 -19.7   0.0  WNW   4.7  Wanshouxigong  \n","4      1023.5 -20.9   0.0   NW   2.6  Wanshouxigong  \n","5      1024.1 -21.7   0.0  WNW   2.5  Wanshouxigong  \n","...       ...   ...   ...  ...   ...            ...  \n","35060  1013.5 -16.2   0.0   NW   2.4  Wanshouxigong  \n","35061  1013.6 -15.1   0.0  WNW   0.9  Wanshouxigong  \n","35062  1014.2 -13.3   0.0   NW   1.1  Wanshouxigong  \n","35063  1014.4 -12.9   0.0  NNW   1.2  Wanshouxigong  \n","35064  1014.1 -15.9   0.0  NNE   1.3  Wanshouxigong  \n","\n","[35064 rows x 17 columns]\n"]}],"source":["raw_df = pd.read_csv('data.csv',index_col='No')\n","\n","#put the columns in a useful order\n","raw_df = raw_df[['PM2.5', 'year', 'month', 'day', 'hour', 'PM10', 'SO2', 'NO2', 'CO',\n","       'O3', 'TEMP', 'PRES', 'DEWP', 'RAIN', 'wd', 'WSPM', 'station']]\n","print(raw_df)"]},{"cell_type":"markdown","metadata":{"id":"oQKg3AFPcYHx"},"source":["Some of the records are missing. We need to handle that before we can easily use the data with most ML tools.\n","\n","### Question 1: Removing missing data [1 mark]\n","\n","We are going to handle this by dropping those rows which have a NaN in one of these columns: ['PM2.5','hour','TEMP','PRES','DEWP','RAIN','wd','WSPM'].\n","\n","Save the result in `nonull_df`.\n","\n","We can use `df.dropna` to do this. Pandas documentation on this method is [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html)."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"k0cYywfFcYHy"},"outputs":[],"source":["#Put answer here\n","nonull_df = raw_df.dropna(axis=0, how='any', subset=['PM2.5', 'hour', 'TEMP', 'PRES', 'DEWP', 'RAIN', 'wd', 'WSPM'])"]},{"cell_type":"markdown","metadata":{"id":"UNruaXp6cYHz"},"source":["To check you've done it correctly, you could use `clean_df.isnull().sum()` to confirm that there are no NaN rows in the columns we're interested in:"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"zPWKkfzHcYH0"},"outputs":[{"data":{"text/plain":["PM2.5        0\n","year         0\n","month        0\n","day          0\n","hour         0\n","PM10        17\n","SO2        242\n","NO2        331\n","CO         866\n","O3         685\n","TEMP         0\n","PRES         0\n","DEWP         0\n","RAIN         0\n","wd           0\n","WSPM         0\n","station      0\n","dtype: int64"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["nonull_df.isnull().sum()"]},{"cell_type":"markdown","metadata":{"id":"8ePRfNOEcYH1"},"source":["### Question 2: Removing unwanted columns [1 mark]\n","\n","Let's remove the columns we're not going to be using. We can use `nonull_df.drop(list_of_column_names, axis=1)` to do this. We will drop: ['year','month','day','PM10','SO2','NO2','CO','O3','station'].\n","\n","Again, feel free to check if it's worked with `clean_df.isnull().sum()`, for example."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"edYxDc5NcYH1"},"outputs":[],"source":["#drop columns year month day PM10 SO2 NO2 CO O3 station\n","clean_df =  nonull_df.drop(['year', 'month', 'day', 'PM10', 'SO2', 'NO2', 'CO', 'O3', 'station'], axis=1)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"0FGsOUWbcYH2"},"outputs":[{"name":"stdout","output_type":"stream","text":["       PM2.5  hour  TEMP    PRES  DEWP  RAIN   wd  WSPM\n","No                                                     \n","1        9.0     0   0.3  1021.9 -19.0   0.0  WNW   2.0\n","2       11.0     1  -0.1  1022.4 -19.3   0.0  WNW   4.4\n","3        8.0     2  -0.6  1022.6 -19.7   0.0  WNW   4.7\n","4        8.0     3  -0.7  1023.5 -20.9   0.0   NW   2.6\n","5        8.0     4  -0.9  1024.1 -21.7   0.0  WNW   2.5\n","...      ...   ...   ...     ...   ...   ...  ...   ...\n","35060   11.0    19  12.5  1013.5 -16.2   0.0   NW   2.4\n","35061   13.0    20  11.6  1013.6 -15.1   0.0  WNW   0.9\n","35062   14.0    21  10.8  1014.2 -13.3   0.0   NW   1.1\n","35063   12.0    22  10.5  1014.4 -12.9   0.0  NNW   1.2\n","35064   13.0    23   8.6  1014.1 -15.9   0.0  NNE   1.3\n","\n","[34284 rows x 8 columns]\n"]}],"source":["#there should be 34284 rows left in your dataframe, and 8 columns (note, this was corrected on 7/11/22)\n","clean_df.shape #=(34284, 8)\n","print(clean_df)"]},{"cell_type":"markdown","metadata":{"id":"UoskxkwycYH3"},"source":["### Question 3: Splitting the dataset [2 marks]\n","\n","Before designing any machine learning model, we need to set aside the test data. We will use the remaining training data for fitting the model. **It is important to remember that the test data has to be set aside before preprocessing**.\n","\n","Any preprocessing that you do has to be done only on the training data and several key statistics need to be saved for the test stage. Separating the dataset into training and test before any preprocessing has happened help us to recreate the real world scenario where we will deploy our system and for which the data will come without any preprocessing.\n","\n","Later we will be performing a grid search to select parameter values. To do this we'll do cross-validation, but rather than split the data into training and validation here we'll split it later. So for now we'll just split into:\n","\n","- The training (and validation) set will have 85% of the total observations, \n","- The test set, the remaining 15%.\n","\n","To avoid unwanted correlations connecting the training and test, we will split these by time. So:\n","\n","- Take the first 85% of the rows from clean_df and put them in train_df, take the remaining 15% of the rows and put them in test_df"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"NjbrkiVucYH5"},"outputs":[],"source":["#Put answer here\n","#split first 85% into train, last 15% into test\n","split_size = int(len(clean_df)*0.85)\n","train_df = clean_df[:split_size]\n","test_df = clean_df[split_size:]\n"]},{"cell_type":"markdown","metadata":{"id":"hn-utn5pcYH6"},"source":["To check the sizes are correct, we can use:"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"lYolbW7RcYH6"},"outputs":[{"data":{"text/plain":["(0.8499883327499709, 0.15001166725002918)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["len(train_df)/len(clean_df),len(test_df)/len(clean_df)"]},{"cell_type":"markdown","metadata":{"id":"xMvSVU3-cYH7"},"source":["# Detour: Lasso Regression\n","\n","Later we will use the sklearn toolkit, but in this section **you will develop your own code** to do the Lasso regression.\n","\n","### Ordinary Least Squares Regression\n","\n","First, let's just perform normal linear regression.\n","\n","We'll use a toy design matrix & labels to use to check our code works. We'll also specify a weight vector too, for testing."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"jRBjUyWgcYH7"},"outputs":[],"source":["X = np.array([[0.0,0],[1,3],[2.2,3]])\n","y = np.array([0.0,1,2])\n","w = np.array([1.0,2])"]},{"cell_type":"markdown","metadata":{"id":"Ou21m-UNcYH7"},"source":["### Question 4: Prediction Function [2 marks]\n","\n","The first task is to write a function to make predictions. Can you complete this function for linear regression, i.e. the predictions for all our points $f(X,\\mathbf{w}) = X \\mathbf{w}$. [corrected: 7/11/22, don't need $X$ tranposed]"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"r2jQlwn0cYH8"},"outputs":[],"source":["def prediction(X,w):\n","    #Put answer here\n","    return np.dot(X,w)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"gEKkPNjgcYH8"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["#You can use this code to check you've written the right function\n","np.all(prediction(X,w)==np.array([0. , 7. , 8.2])) #Should return 'True'"]},{"cell_type":"markdown","metadata":{"id":"wha2MzM7cYH9"},"source":["### Question 5: Objective Function [4 marks]\n","\n","Now we need to write a function that returns the 'error'.\n","We'll just do normal Ordinary Least Squares with linear regression, so if you remember the cost function for that is:\n","\n","$$E = \\sum_{i=1}^N \\big(y_i-f(\\mathbf{x}_i,\\mathbf{w}) \\big)^2$$\n","\n","Where $E$ is the error, $N$ the number of points, $y_i$ is one of the labels, $\\mathbf{x}_i$ is the input for that label, $\\mathbf{w}$ is the weight vector. $f$ is the prediction function you've already written. Or feel free to substitute in $\\mathbf{x}_i^\\top \\mathbf{w}$."]},{"cell_type":"code","execution_count":14,"metadata":{"id":"LKWQ2Y41cYH9"},"outputs":[],"source":["def objective(X,y,w):\n","    \"\"\"\n","    Computes the sum squared error (for us to perform OLS linear regression).\n","    \"\"\"\n","    #Put answer here\n","    #return np.sum((y-(np.dot(X,w)))**2)\n","    f = X@w\n","    resid = y - f\n","    return resid.T @ resid\n","\n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"OBo1zJeicYH-"},"outputs":[{"name":"stdout","output_type":"stream","text":["74.44\n"]},{"data":{"text/plain":["True"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["#You can use this code to check you've written the right function\n","print (objective(X,y,w))\n","objective(X,y,w)==74.44"]},{"cell_type":"markdown","metadata":{"id":"9BhH_-zocYH-"},"source":["### Question 6: Objective Function Gradient [4 marks]\n","\n","Now you need to find the derivative of the objective wrt the parameter vector. You've already done this in lectures, so remember the gradient of the error function (for linear regression) is:\n","\n","$$\\frac{\\partial E}{\\partial \\mathbf{w}} = 2 X^\\top X \\mathbf{w} - 2 X^\\top y$$\n","\n","Add code to do this here:"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"O2TD0-v9cYH-"},"outputs":[{"data":{"text/plain":["array([39.28, 73.2 ])"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["def objective_derivative(X,y,w):\n","    \"\"\"\n","    Computes the derivative of the sum squared error, wrt the parameters.\n","    \"\"\"\n","    #Put answer here   \n","    return 2*X.T@(X@w) - 2*X.T@y\n","objective_derivative(X,y,w)"]},{"cell_type":"markdown","metadata":{"id":"tgvPQtC0cYH_"},"source":["To check your gradients are correct, we can estimate the gradient numerically:"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"9MV54TG3cYH_"},"outputs":[],"source":["def numerical_objective_derivative(X,y,w):\n","    \"\"\"\n","    Computes a numerical approximation to the derivative of the sum squared error, wrt the parameters.\n","    \"\"\"\n","    g = np.zeros_like(w)\n","    for i,wi in enumerate(w):\n","        d = np.zeros_like(w)\n","        d[i]=1e-6\n","        g[i] = (objective(X,y,w+d)-objective(X,y,w-d))/2e-6\n","    return g"]},{"cell_type":"markdown","metadata":{"id":"42SCGt-0cYIA"},"source":["The two gradient vectors should be approximately equal:"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"zuRzN4-xcYIA"},"outputs":[{"data":{"text/plain":["(array([39.28, 73.2 ]), array([39.28, 73.2 ]))"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["objective_derivative(X,y,w),numerical_objective_derivative(X,y,w)"]},{"cell_type":"markdown","metadata":{"id":"-58158ZjcYIA"},"source":["### Question 7: Optimise $\\mathbf{w}$ to minimise the objective [4 marks]\n","\n","Now you need to use the gradient function you've written to maximise $\\mathbf{w}$ using gradient descent.\n","Start with a sensible choice of w. You'll need to loop lots of times (e.g. 1000). At each iteration: compute the gradient and subtract the scaled gradient from the w parameter (you'll need to scale it by the learning rate, of e.g. 0.01)."]},{"cell_type":"code","execution_count":19,"metadata":{"id":"eq9GtHEAcYIB"},"outputs":[],"source":["def optimise_parameters(X,y,startw):\n","    \"\"\"\n","    Returns the w that minimises the objective.\n","    \"\"\"\n","    #Put answer here\n","    w = startw\n","    for i in range(5000):\n","        w = w - objective_derivative(X,y,w)*0.01\n","    return w"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"tmbmC6B5cYIB"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0.83333333 0.05555556]\n"]}],"source":["bestw = optimise_parameters(X,y,w)\n","print(bestw) #print our solution"]},{"cell_type":"markdown","metadata":{"id":"TG713sjPcYIB"},"source":["Let's compare this to the answer provided by sklearn:"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"0qzeBjibcYIC"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0.83333333 0.05555556]\n"]}],"source":["from sklearn import linear_model\n","clf = linear_model.LinearRegression(fit_intercept=False)\n","clf.fit(X,y)\n","print(clf.coef_) #matches the value of w we found above, hopefully!"]},{"cell_type":"markdown","metadata":{"id":"Pn_4IihFcYIC"},"source":["## Lasso Regression\n","\n","### Question 8: New Objective Function [3 marks]\n","\n","We're now going to regularise the regression using $L_1$ regularisation - i.e. Lasso regression.\n","\n","We need a **new objective function**:\n","\n","$$E = \\frac{1}{2N}\\sum_{i=1}^N \\big(y_i-f(\\mathbf{x}_i,\\mathbf{w}) \\big)^2 + \\alpha \\sum_{j=1}^P |w_j|$$\n","\n","This is similar to before (but the first term is now half the mean squared error, rather than the sum squared error). The second term is the L1 regularisation term."]},{"cell_type":"code","execution_count":22,"metadata":{"id":"dl24ORnocYID"},"outputs":[],"source":["def objective_lasso(X,y,w,alpha):\n","    \"\"\"\n","    Computes half the mean squared error, with an additional L1 regularising term. alpha controls the level of regularisation.\n","    \"\"\"\n","    \n","    #Put answer here\n","    hmse = np.sum((y-(np.dot(X,w)))**2)/(len(y)*2)\n","    l1 = alpha * np.sum(np.abs(w))\n","    return hmse + l1"]},{"cell_type":"markdown","metadata":{"id":"FL_qwDiocYID"},"source":["### Question 9: The gradient of the lasso regression objective [3 marks]\n","\n","The tricky bit the derivative of the objective.\n","\n","The first part is similar to before. So, with the regularising term, the derivative is:\n","$$\\frac{\\partial E}{\\partial \\mathbf{w}} = \\frac{1}{N}(X^\\top X \\mathbf{w} - X^\\top y) + \\alpha\\;\\text{sign}(\\mathbf{w})$$\n","\n","where $\\text{sign}(\\mathbf{w})$ returns a vector of the same shape as $\\mathbf{w}$ with +1 if the element is positive and -1 if it's negative. The `np.sign` method does this for you.\n","\n","Have a think about why this is (think about what differentiating the 'absolute' function $|w_j|$ involves - think about what happens when it's positive vs when it's negative. "]},{"cell_type":"code","execution_count":23,"metadata":{"id":"0cCZM2kDcYID"},"outputs":[],"source":["def objective_lasso_derivative(X,y,w,alpha):\n","    \"\"\"\n","    Returns the derivative of the Lasso objective function.\n","    \"\"\"\n","    #Put answer here\n","    return (X.T@(X@w) - X.T@y)/len(y) + (np.sign(w)*alpha)"]},{"cell_type":"markdown","metadata":{"id":"F8BaS8g5cYIE"},"source":["We can check it again, numerically. The two pairs of parameters should be the same:"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"V0aMgXBEcYIE"},"outputs":[{"data":{"text/plain":["(array([ 6.64666667, 12.3       ]), array([ 6.64666667, 12.3       ]))"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["def numerical_objective_lasso_derivative(X,y,w,alpha):\n","    \"\"\"\n","    This finds a numerical approximation to the true gradient\n","    \"\"\"\n","    g = np.zeros_like(w)\n","    for i,wi in enumerate(w):\n","        d = np.zeros_like(w)\n","        d[i]=1e-6\n","        g[i] = (objective_lasso(X,y,w+d,alpha)-objective_lasso(X,y,w-d,alpha))/2e-6\n","    return g\n","\n","objective_lasso_derivative(X,y,w,0.1),numerical_objective_lasso_derivative(X,y,w,0.1)"]},{"cell_type":"markdown","metadata":{"id":"B-m4LjUHcYIE"},"source":["### Question 10: Optimise $\\mathbf{w}$ to minimise the Lasso objective [2 marks]\n","\n","As before we need to optimise to find the optimum value of $\\mathbf{w}$, for this Lasso objective.\n","You'll need to loop lots of times (e.g. 5000). Start with a sensible choice of w. At each iteration: compute the gradient and subtract the scaled gradient from the w parameter (you'll need to scale it by the learning rate, of e.g. 0.05)."]},{"cell_type":"code","execution_count":25,"metadata":{"id":"QpsfOgxFcYIF"},"outputs":[{"data":{"text/plain":["array([0.63888889, 0.14259259])"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["def optimise_parameters_lasso(X,y,startw):\n","    \"\"\"\n","    Returns the w that minimises the Lasso objective.\n","    \"\"\"\n","    #Put answer here\n","    w = startw\n","    alpha = 0.1\n","    for i in range(5000):\n","        w = w - objective_lasso_derivative(X,y,w,alpha)*0.05\n","    return w\n","\n","optimise_parameters_lasso(X,y,w)"]},{"cell_type":"markdown","metadata":{"id":"GK2UPtJHcYIG"},"source":["We can check against the sklearn method:"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"APPYBmk_cYIG"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0.63931263 0.1423666 ]\n"]}],"source":["clf = linear_model.Lasso(alpha=0.1,fit_intercept=False)\n","clf.fit(X,y)\n","print(clf.coef_)"]},{"cell_type":"markdown","metadata":{"id":"438cqaLtcYIH"},"source":["The above result should approximately match the one you computed."]},{"cell_type":"markdown","metadata":{"id":"D5c76jIVcYIH"},"source":["# Back to air pollution\n","\n","### Question 11: One-hot-encoding [4 marks]\n","\n","One of the columns isn't numerical, but instead is a string type: The wind direction. The best way to deal with this is one-hot-encoding.\n","\n","pandas has a tool for doing this: `pd.get_dummies(series, prefix='prefix_to_use')`. In our example the series is: `clean_df.wd`.\n","\n","You'll need to:\n","1. Make the one-hot encoding table using the code above.\n","2. Delete the `wd` column from our table (hint: you did this earlier for other columns).\n","3. Join the one hot data to the table. To do this use something like `dataframe1.join(dataframe2)`.\n","\n"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"4hUiiPE2cYII"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PM2.5</th>\n","      <th>hour</th>\n","      <th>TEMP</th>\n","      <th>PRES</th>\n","      <th>DEWP</th>\n","      <th>RAIN</th>\n","      <th>WSPM</th>\n","      <th>E</th>\n","      <th>ENE</th>\n","      <th>ESE</th>\n","      <th>...</th>\n","      <th>NNW</th>\n","      <th>NW</th>\n","      <th>S</th>\n","      <th>SE</th>\n","      <th>SSE</th>\n","      <th>SSW</th>\n","      <th>SW</th>\n","      <th>W</th>\n","      <th>WNW</th>\n","      <th>WSW</th>\n","    </tr>\n","    <tr>\n","      <th>No</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>9.0</td>\n","      <td>0</td>\n","      <td>0.3</td>\n","      <td>1021.9</td>\n","      <td>-19.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>11.0</td>\n","      <td>1</td>\n","      <td>-0.1</td>\n","      <td>1022.4</td>\n","      <td>-19.3</td>\n","      <td>0.0</td>\n","      <td>4.4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>8.0</td>\n","      <td>2</td>\n","      <td>-0.6</td>\n","      <td>1022.6</td>\n","      <td>-19.7</td>\n","      <td>0.0</td>\n","      <td>4.7</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>8.0</td>\n","      <td>3</td>\n","      <td>-0.7</td>\n","      <td>1023.5</td>\n","      <td>-20.9</td>\n","      <td>0.0</td>\n","      <td>2.6</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>8.0</td>\n","      <td>4</td>\n","      <td>-0.9</td>\n","      <td>1024.1</td>\n","      <td>-21.7</td>\n","      <td>0.0</td>\n","      <td>2.5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>29760</th>\n","      <td>138.0</td>\n","      <td>23</td>\n","      <td>28.2</td>\n","      <td>997.0</td>\n","      <td>24.2</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>29761</th>\n","      <td>145.0</td>\n","      <td>0</td>\n","      <td>27.7</td>\n","      <td>997.1</td>\n","      <td>24.3</td>\n","      <td>0.0</td>\n","      <td>0.9</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>29762</th>\n","      <td>139.0</td>\n","      <td>1</td>\n","      <td>26.9</td>\n","      <td>996.6</td>\n","      <td>24.6</td>\n","      <td>0.0</td>\n","      <td>0.8</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>29763</th>\n","      <td>128.0</td>\n","      <td>2</td>\n","      <td>26.6</td>\n","      <td>996.3</td>\n","      <td>24.8</td>\n","      <td>0.0</td>\n","      <td>0.4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>29764</th>\n","      <td>116.0</td>\n","      <td>3</td>\n","      <td>26.4</td>\n","      <td>996.2</td>\n","      <td>24.6</td>\n","      <td>0.0</td>\n","      <td>0.9</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>29141 rows × 23 columns</p>\n","</div>"],"text/plain":["       PM2.5  hour  TEMP    PRES  DEWP  RAIN  WSPM  E  ENE  ESE  ...  NNW  NW  \\\n","No                                                               ...            \n","1        9.0     0   0.3  1021.9 -19.0   0.0   2.0  0    0    0  ...    0   0   \n","2       11.0     1  -0.1  1022.4 -19.3   0.0   4.4  0    0    0  ...    0   0   \n","3        8.0     2  -0.6  1022.6 -19.7   0.0   4.7  0    0    0  ...    0   0   \n","4        8.0     3  -0.7  1023.5 -20.9   0.0   2.6  0    0    0  ...    0   1   \n","5        8.0     4  -0.9  1024.1 -21.7   0.0   2.5  0    0    0  ...    0   0   \n","...      ...   ...   ...     ...   ...   ...   ... ..  ...  ...  ...  ...  ..   \n","29760  138.0    23  28.2   997.0  24.2   0.0   1.0  0    0    0  ...    0   0   \n","29761  145.0     0  27.7   997.1  24.3   0.0   0.9  0    0    0  ...    0   0   \n","29762  139.0     1  26.9   996.6  24.6   0.0   0.8  1    0    0  ...    0   0   \n","29763  128.0     2  26.6   996.3  24.8   0.0   0.4  0    0    0  ...    0   0   \n","29764  116.0     3  26.4   996.2  24.6   0.0   0.9  1    0    0  ...    0   0   \n","\n","       S  SE  SSE  SSW  SW  W  WNW  WSW  \n","No                                       \n","1      0   0    0    0   0  0    1    0  \n","2      0   0    0    0   0  0    1    0  \n","3      0   0    0    0   0  0    1    0  \n","4      0   0    0    0   0  0    0    0  \n","5      0   0    0    0   0  0    1    0  \n","...   ..  ..  ...  ...  .. ..  ...  ...  \n","29760  0   0    1    0   0  0    0    0  \n","29761  0   0    0    1   0  0    0    0  \n","29762  0   0    0    0   0  0    0    0  \n","29763  0   0    0    0   0  0    0    0  \n","29764  0   0    0    0   0  0    0    0  \n","\n","[29141 rows x 23 columns]"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["def add_wd_onehot(df):\n","    \"\"\"Add new one-hot encoding set of columns, removes the old column it's made from. Returns new dataframe.\"\"\"\n","    #Put answer here\n","    # Get one hot encoding of columns 'vehicleType'\n","    dataframe2 = pd.get_dummies(df['wd'])\n","    \n","    # Drop column as it is now encoded\n","    df = df.drop('wd',axis = 1)\n","\n","    # Join the encoded df\n","    df = df.join(dataframe2)\n","    return df\n","\n","#you could use this code to see if it's worked?\n","train_df_wdencoded = add_wd_onehot(train_df)\n","train_df_wdencoded"]},{"cell_type":"markdown","metadata":{"id":"gR9m46ghcYIJ"},"source":["### Question 12: Standardise the data [3 marks]\n","[note: updated from term 'Normalise' to 'Standardise' on 7/11/22. For clarity, I want the mean to be zero and the standard deviation to be one].\n","\n","Now we need to standardise [edit: corrected] the data.\n","\n","You could manipulate just some columns by using, for example: `df.iloc[:,1:]` - this returns a dataframe that consists of all but the first column.\n","\n","Feel free to use either tools from `sklearn.preprocessing` or standardise [edit: corrected on 7/11/22] it by, for example, using the mean and the standard deviation of the columns by calling `some_dataframe.mean()` or `some_dataframe.std()`."]},{"cell_type":"code","execution_count":28,"metadata":{"id":"ylCMFcVccYIJ"},"outputs":[{"name":"stdout","output_type":"stream","text":["       PM2.5      hour      TEMP      PRES      DEWP      RAIN      WSPM  \\\n","No                                                                         \n","1        9.0 -1.657991 -1.244548  1.080110 -1.612190 -0.084708  0.194043   \n","2       11.0 -1.513672 -1.280171  1.128008 -1.634231 -0.084708  2.157165   \n","3        8.0 -1.369353 -1.324700  1.147168 -1.663619 -0.084708  2.402555   \n","4        8.0 -1.225034 -1.333606  1.233384 -1.751782 -0.084708  0.684824   \n","5        8.0 -1.080716 -1.351418  1.290862 -1.810558 -0.084708  0.603027   \n","...      ...       ...       ...       ...       ...       ...       ...   \n","29760  138.0  1.661344  1.240186 -1.305218  1.561690 -0.084708 -0.623924   \n","29761  145.0 -1.657991  1.195657 -1.295639  1.569037 -0.084708 -0.705721   \n","29762  139.0 -1.513672  1.124410 -1.343537  1.591078 -0.084708 -0.787518   \n","29763  128.0 -1.369353  1.097692 -1.372276  1.605772 -0.084708 -1.114705   \n","29764  116.0 -1.225034  1.079881 -1.381855  1.591078 -0.084708 -0.705721   \n","\n","              E      ENE       ESE  ...       NNW        NW         S  \\\n","No                                  ...                                 \n","1     -0.294383 -0.30128 -0.227165  ... -0.181767 -0.279390 -0.214779   \n","2     -0.294383 -0.30128 -0.227165  ... -0.181767 -0.279390 -0.214779   \n","3     -0.294383 -0.30128 -0.227165  ... -0.181767 -0.279390 -0.214779   \n","4     -0.294383 -0.30128 -0.227165  ... -0.181767  3.579232 -0.214779   \n","5     -0.294383 -0.30128 -0.227165  ... -0.181767 -0.279390 -0.214779   \n","...         ...      ...       ...  ...       ...       ...       ...   \n","29760 -0.294383 -0.30128 -0.227165  ... -0.181767 -0.279390 -0.214779   \n","29761 -0.294383 -0.30128 -0.227165  ... -0.181767 -0.279390 -0.214779   \n","29762  3.396933 -0.30128 -0.227165  ... -0.181767 -0.279390 -0.214779   \n","29763 -0.294383 -0.30128 -0.227165  ... -0.181767 -0.279390 -0.214779   \n","29764  3.396933 -0.30128 -0.227165  ... -0.181767 -0.279390 -0.214779   \n","\n","             SE       SSE       SSW        SW         W       WNW       WSW  \n","No                                                                           \n","1     -0.184071 -0.172282 -0.286043 -0.332946 -0.292866  3.325909 -0.293143  \n","2     -0.184071 -0.172282 -0.286043 -0.332946 -0.292866  3.325909 -0.293143  \n","3     -0.184071 -0.172282 -0.286043 -0.332946 -0.292866  3.325909 -0.293143  \n","4     -0.184071 -0.172282 -0.286043 -0.332946 -0.292866 -0.300670 -0.293143  \n","5     -0.184071 -0.172282 -0.286043 -0.332946 -0.292866  3.325909 -0.293143  \n","...         ...       ...       ...       ...       ...       ...       ...  \n","29760 -0.184071  5.804452 -0.286043 -0.332946 -0.292866 -0.300670 -0.293143  \n","29761 -0.184071 -0.172282  3.495979 -0.332946 -0.292866 -0.300670 -0.293143  \n","29762 -0.184071 -0.172282 -0.286043 -0.332946 -0.292866 -0.300670 -0.293143  \n","29763 -0.184071 -0.172282 -0.286043 -0.332946 -0.292866 -0.300670 -0.293143  \n","29764 -0.184071 -0.172282 -0.286043 -0.332946 -0.292866 -0.300670 -0.293143  \n","\n","[29141 rows x 23 columns]\n"]}],"source":["from sklearn.preprocessing import StandardScaler\n","def standardise(df):\n","    \"\"\"\n","    Returns a new dataframe in which all but the PM2.5 columns are standardised (i.e. have a mean of zero and standard deviation of 1)\n","    [note: the function name used to be 'normalise' but was modified for clarity]\n","\n","    [addition: 7/11/22\n","    Think about if you want to standardise using the *training* data's mean and standard deviation (for the test data).\n","    \"\"\"\n","    #Put answer here\n","    #manually standardise\n","    #temp = df.iloc[:,0]\n","    #df = df.drop(df.columns[0],axis=1)\n","    #df = (df - df.mean())/df.std()\n","    #df.insert(0, 'PM2.5', temp)\n","    \n","    #using sklearn\n","    df1 = df.iloc[:,0]\n","    df2 = df.iloc[:,1:]\n","    scaler = StandardScaler()\n","    df2 = pd.DataFrame(scaler.fit_transform(df2), columns = df.columns[1:], index = df.index)\n","    df2.insert(0, 'PM2.5', df1)\n","    df = df2\n","    return df\n","\n","train_df_preprocessed = standardise(add_wd_onehot(train_df))\n","test_df_preprocessed = standardise(add_wd_onehot(test_df))\n","#print(add_wd_onehot(train_df))\n","print(train_df_preprocessed)\n"]},{"cell_type":"markdown","metadata":{"id":"7cUc5TFLcYIJ"},"source":["Here we put the training and test inputs (X) and outputs (y) into four variables:"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"9reEdwwjcYIJ"},"outputs":[],"source":["X = train_df_preprocessed.iloc[:,1:].to_numpy()\n","y = train_df_preprocessed.iloc[:,0].to_numpy()\n","\n","Xtest = test_df_preprocessed.iloc[:,1:].to_numpy()\n","ytest = test_df_preprocessed.iloc[:,0].to_numpy()"]},{"cell_type":"markdown","metadata":{"id":"7Cbnx-YWcYIK"},"source":["We can use the same code we wrote before, using the Lasso from sklearn to fit the data. Here we'll turn fit_intercept on, as we've not added a '1's column to our design matrix.\n","\n","So feel free to use:\n","```\n","clf = linear_model.Lasso(alpha=0.1,fit_intercept=True)\n","clf.fit(X,y)\n","```\n","\n","### Question 13: Finding the RMSE of the Lasso regressor predictions [2 marks]\n","\n","Next compute the **RMSE** of the predictions for (a) the training data and (b) the test data.\n","The RMSE (root mean squared error) could be computed, for example with:\n","\n","```np.sqrt(np.mean((predicted_values-true_values)**2))```"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"m6HCR6qWcYIK"},"outputs":[{"name":"stdout","output_type":"stream","text":["72.40451930761152 88.63540499735817\n"]}],"source":["#Put answer here\n","clf = linear_model.Lasso(alpha=0.1,fit_intercept=True)\n","clf.fit(X,y)\n","#a\n","predicted_values = clf.predict(X)\n","true_values = y\n","rmse_lasso_train = np.sqrt(np.mean((predicted_values - true_values)**2))\n","#b\n","predicted_values_test = clf.predict(Xtest)\n","true_values_test = ytest\n","rmse_lasso_test = np.sqrt(np.mean((predicted_values_test - true_values_test)**2))\n","print(rmse_lasso_train,rmse_lasso_test)"]},{"cell_type":"markdown","metadata":{"id":"QzqNV3ticYIL"},"source":["We can compare this to the standard deviation of the data, we should do better than that!"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"NYXvIHwMcYIL"},"outputs":[{"data":{"text/plain":["(82.43136001277581, 103.25109103178704)"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["np.std(y), np.std(ytest)"]},{"cell_type":"markdown","metadata":{"id":"qMNAk_HrcYIL"},"source":["### Question 14: Random Forest [8 marks]\n","\n","The final step is to use a random forest regressor.\n","\n","If we use the default random forest regressor, we find we get considerable over-fitting. So we need to explore different parameters. We will use a cross-validated grid search over the parameters:\n","\n","- max_features: The number of features to consider when looking for the best split (i.e. controls subsampling), *from 1 to the number of features* in 4 steps (e.g. use `np.linspace`)\n","- n_estimators: The number of trees in the forest, from 10 to 100 in 4 steps.\n","- max_samples : the number of samples to draw from to train each base estimator, from 0.1 to 0.9 in 4 steps.\n","\n","We will use `GridSearchCV`.\n","\n","Have a look at the documentation for this, the three parameters we need to specify are:\n","\n","- the 'estimator': an **INSTANCE** of RandomForestRegressor.\n","- param_grid: a **DICTIONARY**, each item is the title of the parameter, and equals an array of the values we need to test. For exmaple, one of the items might be `{'max_samples': np.linspace(0.1, 0.9, 5)}`.\n","- You'll need to think carefully how to make the lists for the `max_features` and `n_estimators` as these both need to be (positive) integers. E.g. use `.astype(int)`."]},{"cell_type":"code","execution_count":32,"metadata":{"id":"vObtrvKxcYIM"},"outputs":[{"data":{"text/plain":["GridSearchCV(estimator=RandomForestRegressor(),\n","             param_grid={'max_features': array([ 1,  8, 15, 22]),\n","                         'max_samples': array([0.1       , 0.36666667, 0.63333333, 0.9       ]),\n","                         'n_estimators': array([ 10,  40,  70, 100])},\n","             scoring='neg_root_mean_squared_error')"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["#Put answer here:\n","#1. Create a grid of parameter values for n_estimators, max_features and max_samples,\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.model_selection import GridSearchCV\n","n_estimators_opts = np.linspace(10, 100, 4, dtype=int)\n","max_features_opts = np.linspace(1, X.shape[1], 4, dtype=int)\n","max_samples_opts = np.linspace(0.1, 0.9, 4)\n","param_grid = {'n_estimators': n_estimators_opts, 'max_features': max_features_opts, 'max_samples': max_samples_opts}\n","#2. Create a GridSearchCV object, using the random forest regressor:\n","rf = RandomForestRegressor()\n","grid_regression = GridSearchCV(rf, param_grid, scoring='neg_root_mean_squared_error')\n","\n","#Note: Because there is so much training data, using the full dataset takes too long. So here we'll just use 10%\n","np.random.seed(42)\n","idx = np.sort(np.random.choice(len(X), size=int(len(X)*0.1), replace=False))\n","#3. Fit to training data in (the subset of) X and y\n","grid_regression.fit(X[idx,:],y[idx])"]},{"cell_type":"markdown","metadata":{"id":"HUuX3bOYcYIM"},"source":["Here we print the best parameters from the grid search (on the training/validation cross-validation run):"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"ShaXFzjQcYIN"},"outputs":[{"name":"stdout","output_type":"stream","text":["Best n_estimator 40\n","Best max features 8\n","Best max samples 0.6333333333333333\n"]}],"source":["best_n_estimators = grid_regression.best_params_['n_estimators']\n","best_max_features = grid_regression.best_params_['max_features']\n","best_max_samples = grid_regression.best_params_['max_samples']\n","print('Best n_estimator', best_n_estimators)\n","print('Best max features', best_max_features)\n","print('Best max samples', best_max_samples)"]},{"cell_type":"markdown","metadata":{"id":"dczjj2ercYIN"},"source":["### Question 15: RMSE for the Random Forest Regressor [1 mark]\n","\n","Finally compute the RMSE for the training and test data:"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"1SAtgw13cYIO"},"outputs":[{"name":"stdout","output_type":"stream","text":["31.33178385455363 80.29388321949091\n"]}],"source":["#Put answer here\n","rf = RandomForestRegressor(n_estimators=best_n_estimators, max_samples=best_max_samples, max_features=best_max_features,random_state=42)\n","rf.fit(X,y)\n","#a\n","predicted_values = rf.predict(X)\n","true_values = y\n","rmse_rf_train = np.sqrt(np.mean((predicted_values - true_values)**2))\n","#b\n","predicted_values_test = rf.predict(Xtest)\n","true_values_test = ytest\n","rmse_rf_test = np.sqrt(np.mean((predicted_values_test - true_values_test)**2))\n","print(rmse_rf_train,rmse_rf_test)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"S_j3g8cycYIP"},"source":["We can compare this to the standard deviations for the two sets of data."]},{"cell_type":"code","execution_count":35,"metadata":{"id":"FRjHEn1pcYIQ"},"outputs":[{"data":{"text/plain":["(82.43136001277581, 103.25109103178704)"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["np.std(y), np.std(ytest)"]},{"cell_type":"markdown","metadata":{"id":"wpY-t0TfcYIQ"},"source":["### Question 16: Did the random forest do better than lasso regression? [1 mark]\n","\n"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"KOBVQyVpcYIR"},"outputs":[],"source":["#Put answer here\n","#Random forest did better than lasso regression for the training and tetest data"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12 (main, Jun  1 2022, 06:34:44) \n[Clang 12.0.0 ]"},"vscode":{"interpreter":{"hash":"8afbc88b4aa8699ecb60f3d049919b2afbf00c7f7e60aeb4638f76858e162b8e"}}},"nbformat":4,"nbformat_minor":0}
